"""
Exemplo demonstrando o workflow JSON â†’ TOON â†’ JSON

Este exemplo mostra como:
1. Trabalhar com dados em JSON programaticamente
2. Converter para TOON antes de enviar para LLM
3. Receber resposta e converter de volta para JSON
"""

from utils.toon_converter import TOONConverter, TOONPromptBuilder
import json

def example_1_basic_conversion():
    """Exemplo 1: ConversÃ£o bÃ¡sica JSON â†’ TOON"""
    print("\n" + "="*60)
    print("EXEMPLO 1: ConversÃ£o BÃ¡sica JSON â†’ TOON")
    print("="*60 + "\n")
    
    # Dados em JSON (como trabalhamos programaticamente)
    user_data = {
        "name": "JoÃ£o Silva",
        "query": "Quanto Ã© 128 vezes 46?",
        "timestamp": "2024-01-15T10:30:00",
        "metadata": {
            "session_id": "abc123",
            "platform": "web"
        }
    }
    
    print("ðŸ“¦ Dados originais (JSON):")
    print(json.dumps(user_data, indent=2, ensure_ascii=False))
    
    # Converte para TOON antes de enviar para LLM
    toon_format = TOONConverter.json_to_toon(user_data)
    
    print("\nðŸ“¤ Formato TOON (enviado para LLM):")
    print(toon_format)
    
    print("\nâœ… Vantagem: LLMs processam TOON melhor que JSON puro")

def example_2_structured_output():
    """Exemplo 2: SaÃ­da estruturada com schema"""
    print("\n" + "="*60)
    print("EXEMPLO 2: SaÃ­da Estruturada com Schema")
    print("="*60 + "\n")
    
    # Definimos o schema da resposta esperada (em JSON)
    output_schema = {
        "category": "string (CALCULATOR|RAG|DATETIME|DIRECT)",
        "confidence": "float (0.0-1.0)",
        "reasoning": "string",
        "suggested_tools": "array of strings"
    }
    
    # Input data
    input_data = {
        "user_query": "Me fale sobre Large Language Models",
        "context": "technical_documentation"
    }
    
    # ConstrÃ³i prompt estruturado em TOON
    structured_prompt = TOONPromptBuilder.build_structured_prompt(
        task="Classify the user query and suggest appropriate tools",
        input_data=input_data,
        output_schema=output_schema
    )
    
    print("ðŸ“¤ Prompt enviado para LLM (TOON):")
    print(structured_prompt)
    
    print("\nðŸ’¡ O LLM receberÃ¡ um prompt claramente estruturado")
    print("ðŸ’¡ E responderÃ¡ em JSON matching o schema")

def example_3_tool_call_workflow():
    """Exemplo 3: Workflow completo de chamada de ferramenta"""
    print("\n" + "="*60)
    print("EXEMPLO 3: Workflow Completo de Tool Call")
    print("="*60 + "\n")
    
    # Passo 1: Router classifica (trabalha em JSON)
    router_input = {
        "query": "Calcule a raiz quadrada de 144",
        "user_id": "user_123"
    }
    
    print("1ï¸âƒ£ Router recebe input (JSON):")
    print(json.dumps(router_input, indent=2, ensure_ascii=False))
    
    # Passo 2: Converte para TOON e envia para LLM
    toon_input = TOONConverter.json_to_toon(router_input)
    print("\n2ï¸âƒ£ Convertido para TOON:")
    print(toon_input)
    
    # Passo 3: LLM responde (simulado)
    llm_response_json = {
        "category": "CALCULATOR",
        "confidence": 0.98,
        "tool_call": {
            "name": "calculator",
            "arguments": {
                "expression": "sqrt(144)"
            }
        }
    }
    
    print("\n3ï¸âƒ£ LLM responde em JSON:")
    print(json.dumps(llm_response_json, indent=2, ensure_ascii=False))
    
    # Passo 4: Preparamos a tool call em TOON
    tool_call_toon = TOONConverter.tool_call_to_toon(
        llm_response_json["tool_call"]["name"],
        llm_response_json["tool_call"]["arguments"]
    )
    
    print("\n4ï¸âƒ£ Tool call formatado em TOON:")
    print(tool_call_toon)
    
    # Passo 5: Executamos a tool e trabalhamos em JSON
    tool_result = {"result": 12.0, "status": "success"}
    
    print("\n5ï¸âƒ£ Tool retorna resultado (JSON):")
    print(json.dumps(tool_result, indent=2, ensure_ascii=False))
    
    print("\nâœ… Todo o workflow mantÃ©m dados em JSON")
    print("âœ… TOON Ã© usado apenas na comunicaÃ§Ã£o com LLM")

def example_4_real_world_scenario():
    """Exemplo 4: CenÃ¡rio real com Router Agent"""
    print("\n" + "="*60)
    print("EXEMPLO 4: CenÃ¡rio Real - Router Agent")
    print("="*60 + "\n")
    
    # AplicaÃ§Ã£o trabalha em JSON
    application_state = {
        "user_query": "Qual Ã© a diferenÃ§a entre 2024-12-31 e 2024-01-01?",
        "session": {
            "id": "session_456",
            "history_count": 5
        },
        "user_preferences": {
            "language": "pt-BR",
            "timezone": "America/Sao_Paulo"
        }
    }
    
    print("ðŸ“± Estado da aplicaÃ§Ã£o (JSON):")
    print(json.dumps(application_state, indent=2, ensure_ascii=False))
    
    # Preparamos para enviar ao Router
    router_prompt_data = {
        "query": application_state["user_query"],
        "context": {
            "has_history": application_state["session"]["history_count"] > 0,
            "language": application_state["user_preferences"]["language"]
        }
    }
    
    # Schema esperado da resposta
    response_schema = {
        "category": "string",
        "confidence": "float",
        "reasoning": "string"
    }
    
    # ConstrÃ³i prompt TOON
    toon_prompt = TOONPromptBuilder.build_structured_prompt(
        task="Classify this query into the most appropriate category",
        input_data=router_prompt_data,
        output_schema=response_schema,
        examples=[
            {
                "input": {"query": "What's the difference between two dates?"},
                "output": {
                    "category": "DATETIME",
                    "confidence": 0.95,
                    "reasoning": "Date calculation detected"
                }
            }
        ]
    )
    
    print("\nðŸ“¤ Prompt enviado ao LLM (TOON):")
    print(toon_prompt[:500] + "...")
    
    # LLM responde
    llm_response = {
        "category": "DATETIME",
        "confidence": 0.97,
        "reasoning": "User is asking to calculate difference between two dates"
    }
    
    print("\nðŸ“¥ Resposta do LLM (JSON parseado):")
    print(json.dumps(llm_response, indent=2, ensure_ascii=False))
    
    # AplicaÃ§Ã£o continua trabalhando em JSON
    application_state["routing_decision"] = llm_response
    application_state["next_agent"] = llm_response["category"].lower() + "_agent"
    
    print("\nðŸ“± Estado atualizado da aplicaÃ§Ã£o (JSON):")
    print(json.dumps(application_state, indent=2, ensure_ascii=False))
    
    print("\n" + "="*60)
    print("RESUMO DO WORKFLOW")
    print("="*60)
    print("âœ… AplicaÃ§Ã£o trabalha 100% em JSON")
    print("âœ… TOON usado apenas para comunicaÃ§Ã£o com LLM")
    print("âœ… Parsing de volta para JSON automÃ¡tico")
    print("âœ… Estrutura clara e type-safe")

if __name__ == "__main__":
    # Executa todos os exemplos
    example_1_basic_conversion()
    example_2_structured_output()
    example_3_tool_call_workflow()
    example_4_real_world_scenario()
    
    print("\n\n" + "="*60)
    print("ðŸŽ¯ CONCLUSÃƒO")
    print("="*60)
    print("""
O workflow JSON â†’ TOON â†’ JSON oferece:

1. âœ… Trabalho programÃ¡tico em JSON (type-safe, fÃ¡cil manipular)
2. âœ… ComunicaÃ§Ã£o otimizada com LLM (TOON Ã© mais legÃ­vel)
3. âœ… Parsing automÃ¡tico de volta para JSON
4. âœ… Schemas claros para validaÃ§Ã£o
5. âœ… Melhor performance e accuracy do LLM

Este Ã© o padrÃ£o recomendado para aplicaÃ§Ãµes profissionais!
    """)